{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "349aa44d-8c3c-486e-b864-2fa7c9ea9d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CamembertTokenizer, CamembertForSequenceClassification\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b677ad69-1d88-401c-866a-694b99ef0e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CamembertTokenizer, CamembertForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81aad5af-d0e5-451d-8cb2-67c486c931c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer et modèle chargés !\n"
     ]
    }
   ],
   "source": [
    "tokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")\n",
    "\n",
    "# Charger le modèle (3 labels : entailment, neutral, contradiction)\n",
    "model = CamembertForSequenceClassification.from_pretrained(\"camembert-base\", num_labels=3)\n",
    "\n",
    "print(\"Tokenizer et modèle chargés !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0558d650-80fe-4f61-b3ea-9bdd52449cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fichier trouvé\n",
      "Dataset sauvegardé dans ../Youdas/dataset_pos.json avec 192 exemples.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import json\n",
    "\n",
    "# 1. Charger le modèle spaCy pour le français\n",
    "nlp = spacy.load(\"fr_core_news_md\")\n",
    "\n",
    "# 2. Définir la fonction d'annotation\n",
    "def annotate_pos(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text for token in doc]\n",
    "    pos_tags = [token.pos_ for token in doc]\n",
    "    return {\"tokens\": tokens, \"pos_tags\": pos_tags}\n",
    "\n",
    "# 3. Lire le fichier texte et annoter chaque ligne\n",
    "def process_text_file(input_file, output_file):\n",
    "    dataset = []\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        print('fichier trouvé')\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:  # Ignorer les lignes vides\n",
    "                annotated = annotate_pos(line)\n",
    "                dataset.append(annotated)\n",
    "\n",
    "    # 4. Sauvegarder le résultat au format JSON\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(dataset, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"Dataset sauvegardé dans {output_file} avec {len(dataset)} exemples.\")\n",
    "\n",
    "# 5. Exécuter le script\n",
    "input_file = \"../Youdas/test1.txt\"  # Remplace par le chemin de ton fichier\n",
    "output_file = \"../Youdas/dataset_pos.json\"  # Fichier de sortie\n",
    "process_text_file(input_file, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6ed5fdc8-aa5b-48fd-9d42-06f966846bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting fr-core-news-md==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_md-3.8.0/fr_core_news_md-3.8.0-py3-none-any.whl (45.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 MB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: fr-core-news-md\n",
      "Successfully installed fr-core-news-md-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('fr_core_news_md')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "#import spacy.cli\n",
    "#spacy.cli.download(\"fr_core_news_md\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ba029b5a-da9e-4f09-9d09-44974a6d9c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle chargé avec succès !\n"
     ]
    }
   ],
   "source": [
    "#import spacy\n",
    "#nlp = spacy.load(\"fr_core_news_md\")\n",
    "#print(\"Modèle chargé avec succès !\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3999afdf-a746-423c-953f-495dc88a8864",
   "metadata": {},
   "source": [
    "### preparer les données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4b7b56d-5630-4691-8743-c126d68922b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['Si', 'vous', 'avez', 'raté', 'ou', 'supprimé', 'une', 'newsletter', ',', 'il', 'faudra', 'patienter', 'un', 'peu', 'pour', 'attendre', 'la', 'prochaine', '^^'], 'pos_tags': ['SCONJ', 'PRON', 'AUX', 'VERB', 'CCONJ', 'VERB', 'DET', 'NOUN', 'PUNCT', 'PRON', 'VERB', 'VERB', 'DET', 'ADV', 'ADP', 'VERB', 'DET', 'ADJ', 'PUNCT']}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Column 'test' doesn't exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     17\u001b[39m eval_dataset = dataset[\u001b[33m'\u001b[39m\u001b[33mtest\u001b[39m\u001b[33m'\u001b[39m].train_test_split(test_size=\u001b[32m0.5\u001b[39m, seed=\u001b[32m42\u001b[39m)\n\u001b[32m     18\u001b[39m eval_dataset = eval_dataset[\u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m test_dataset = \u001b[43meval_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtest\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTrain: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Eval: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(eval_dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Test: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(test_dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/datasets/arrow_dataset.py:2875\u001b[39m, in \u001b[36mDataset.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   2873\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m   2874\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._format_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._format_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33marrow\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mpandas\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mpolars\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m2875\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2876\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem(key)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/datasets/arrow_dataset.py:658\u001b[39m, in \u001b[36mColumn.__init__\u001b[39m\u001b[34m(self, source, column_name)\u001b[39m\n\u001b[32m    656\u001b[39m \u001b[38;5;28mself\u001b[39m.column_name = column_name\n\u001b[32m    657\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(source.features, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m column_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m source.features:\n\u001b[32m--> \u001b[39m\u001b[32m658\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mColumn \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolumn_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m doesn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt exist.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    659\u001b[39m \u001b[38;5;28mself\u001b[39m.features = source.features[column_name]\n",
      "\u001b[31mValueError\u001b[39m: Column 'test' doesn't exist."
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import json\n",
    "\n",
    "# Charger le JSON\n",
    "with open(\"../Youdas/dataset_pos.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Créer un Dataset Hugging Face\n",
    "dataset = Dataset.from_list(data)\n",
    "\n",
    "# Afficher un exemple\n",
    "print(dataset[0])\n",
    "\n",
    "# Splitter en train/validation/test (80/10/10)\n",
    "dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = dataset['train']\n",
    "eval_dataset = dataset['test'].train_test_split(test_size=0.5, seed=42)\n",
    "eval_dataset = eval_dataset['train']\n",
    "test_dataset = eval_dataset['test']\n",
    "\n",
    "print(f\"Train: {len(train_dataset)}, Eval: {len(eval_dataset)}, Test: {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6469ba26-f157-4671-bbc3-f7de2106bdac",
   "metadata": {},
   "source": [
    "## Encoder les tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58305d9-67af-4586-b1fc-d56fd29d1819",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Récupérer tous les tags uniques\n",
    "all_tags = set(tag for example in train_dataset[\"pos_tags\"] for tag in example)\n",
    "tag2id = {tag: id for id, tag in enumerate(sorted(all_tags))}\n",
    "id2tag = {id: tag for tag, id in tag2id.items()}\n",
    "num_labels = len(tag2id)\n",
    "\n",
    "print(f\"Nombre de tags uniques : {num_labels}\")\n",
    "print(\"Exemple de mapping :\", {k: tag2id[k] for k in list(tag2id)[:5]})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2d44cf-0259-468d-a626-5ed8f04fd665",
   "metadata": {},
   "source": [
    "## Tokenisation et alignement des labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a39c84f-d7b1-40db-b8f6-cdd454048eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CamembertTokenizer\n",
    "\n",
    "tokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"pos_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:  # Token spécial ([CLS], [SEP], [PAD])\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Nouveau mot\n",
    "                label_ids.append(tag2id[label[word_idx]])\n",
    "            else:  # Sous-mot du même mot\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Appliquer à tout le dataset\n",
    "tokenized_train = train_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "tokenized_eval = eval_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "tokenized_test = test_dataset.map(tokenize_and_align_labels, batched=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8330b6-68b1-4737-b5b8-7be316d004c5",
   "metadata": {},
   "source": [
    "## Fine-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3e816d-7049-453c-a869-bf0bc8a87a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CamembertForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = CamembertForTokenClassification.from_pretrained(\"camembert-base\", num_labels=num_labels)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_eval,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4e0966-14f8-47af-827a-350831c95e5b",
   "metadata": {},
   "source": [
    "## evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ad26f5-83f3-4be5-9b71-7c1e7b99e49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import classification_report\n",
    "\n",
    "predictions = trainer.predict(tokenized_test)\n",
    "preds = np.argmax(predictions.predictions, axis=2)\n",
    "\n",
    "# Convertir les IDs en tags\n",
    "pred_labels = [[id2tag[p] for p in pred if p != -100] for pred in preds]\n",
    "true_labels = [[id2tag[l] for l in label if l != -100] for label in tokenized_test[\"labels\"]]\n",
    "\n",
    "print(classification_report(true_labels, pred_labels))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
