{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26d72966-6a71-40a0-afeb-b1d7b4164fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CamembertTokenizer, CamembertForSequenceClassification\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5482149f-8f81-4ea5-a600-1df9e835f330",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CamembertTokenizer, CamembertForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import spacy\n",
    "import json\n",
    "\n",
    "from datasets import load_dataset, Dataset\n",
    "from itertools import islice\n",
    "from transformers import CamembertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c92ac70e-0136-4e01-88e9-5c172c5b7e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre total de lignes : 6689783\n",
      "Nombre de lignes à extraire : 1337956\n",
      "Extraction terminée : 1337956 lignes ont été écrites dans ../Youdas/echantillon_20_pourcent.txt.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"import random\n",
    "\n",
    "input_file = '../dataset_g5/fr_part_1.txt'\n",
    "output_file = '../Youdas/echantillon_10_pourcent.txt'\n",
    "percentage = 0.4 # 10%\n",
    "\n",
    "with open(input_file, 'r') as infile, open(output_file, 'w') as outfile:\n",
    "    for line in infile:\n",
    "        if random.random() < percentage:\n",
    "            outfile.write(line)\n",
    "            \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "input_file = '../dataset_g5/fr_part_1.txt'\n",
    "output_file = '../Youdas/echantillon_20_pourcent.txt'\n",
    "percentage = 0.2  \n",
    "\n",
    "# Calculer le nombre de lignes à extraire\n",
    "total_lines = sum(1 for _ in open(input_file, 'r'))\n",
    "num_lines_to_extract = int(total_lines * percentage)\n",
    "\n",
    "print(f\"Nombre total de lignes : {total_lines}\")\n",
    "print(f\"Nombre de lignes à extraire : {num_lines_to_extract}\")\n",
    "\n",
    "\n",
    "with open(input_file, 'r') as infile, open(output_file, 'w') as outfile:\n",
    "    for i, line in enumerate(infile):\n",
    "        if i < num_lines_to_extract:\n",
    "            outfile.write(line)\n",
    "        else:\n",
    "            break \n",
    "\n",
    "print(f\"Extraction terminée : {num_lines_to_extract} lignes ont été écrites dans {output_file}.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ab14c3-798f-4300-9049-95b6330c36cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")\n",
    "\n",
    "# Charger le modèle (3 labels : entailment, neutral, contradiction)\n",
    "model = CamembertForSequenceClassification.from_pretrained(\"camembert-base\", num_labels=3)\n",
    "\n",
    "print(\"Tokenizer et modèle chargés !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8dad3b9-f295-497a-b685-c69d880f6cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "nlp = spacy.load(\"fr_core_news_md\")\n",
    "\n",
    "# 2. Définir la fonction d'annotation\n",
    "def annotate_pos(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text for token in doc]\n",
    "    pos_tags = [token.pos_ for token in doc]\n",
    "    return {\"tokens\": tokens, \"pos_tags\": pos_tags}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "170ca17f-58f6-4cf1-be0d-0d7a1e3556ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text_file(input_file, output_file):\n",
    "    dataset = []\n",
    "\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = (line.strip() for line in f if line.strip())\n",
    "\n",
    "        for doc in nlp.pipe(lines, batch_size=2000):\n",
    "            \n",
    "            dataset.append({\n",
    "                \"tokens\": [token.text for token in doc],\n",
    "                \"pos_tags\": [token.pos_ for token in doc]\n",
    "            })\n",
    "\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(dataset, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"Dataset sauvegardé dans {output_file} avec {len(dataset)} exemples.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c82b5654-62ce-4ad9-93b0-ca0ec85db91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sauvegardé dans ../Youdas/echantillon_20_pourcent.json avec 1337956 exemples.\n"
     ]
    }
   ],
   "source": [
    "#input_file = \"../Youdas/echantillon_20_pourcent.txt\"  \n",
    "#output_file = \"../Youdas/echantillon_20_pourcent.json\"  # Fichier de sortie\n",
    "#process_text_file(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67e8bc19-6d5e-4bf1-9204-0aa2bec379ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Conversion en Arrow...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97d66e3c090a4d9da31db8a4928e6647",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/133796 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f301c222b3b494bac052dfe137e7664",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/133796 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e59511d75b948bcab8fbc1ad02a74dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/133796 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Terminé\n",
      "Train: 133796\n",
      "Eval : 133796\n",
      "Test : 133796\n"
     ]
    }
   ],
   "source": [
    "\n",
    "JSON_PATH = \"../Youdas/echantillon_20_pourcent.json\"\n",
    "OUT_DIR = \"./postagging_dataset_20%\"\n",
    "SEED = 42\n",
    "BUFFER_SIZE = 100_000\n",
    "\n",
    "def get_stream():\n",
    "    ds = load_dataset(\n",
    "        \"json\",\n",
    "        data_files=JSON_PATH,\n",
    "        split=\"train\",\n",
    "        streaming=True\n",
    "    )\n",
    "    ds = ds.shuffle(seed=SEED, buffer_size=BUFFER_SIZE)\n",
    "    return ds\n",
    "\n",
    "def train_gen():\n",
    "    return islice(get_stream(), 0, None, 10)\n",
    "\n",
    "def eval_gen():\n",
    "    return islice(get_stream(), 1, None, 10)\n",
    "\n",
    "def test_gen():\n",
    "    return islice(get_stream(), 2, None, 10)\n",
    "\n",
    "print(\" Conversion en Arrow...\")\n",
    "\n",
    "train_dataset = Dataset.from_generator(train_gen)\n",
    "eval_dataset  = Dataset.from_generator(eval_gen)\n",
    "test_dataset  = Dataset.from_generator(test_gen)\n",
    "\n",
    "train_dataset.save_to_disk(f\"{OUT_DIR}/train\")\n",
    "eval_dataset.save_to_disk(f\"{OUT_DIR}/eval\")\n",
    "test_dataset.save_to_disk(f\"{OUT_DIR}/test\")\n",
    "\n",
    "print(\" Terminé\")\n",
    "print(f\"Train: {len(train_dataset)}\")\n",
    "print(f\"Eval : {len(eval_dataset)}\")\n",
    "print(f\"Test : {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37567c75-b0fd-4c0d-8d29-dea8e661e050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping des labels : {0: 'ADJ', 1: 'ADP', 2: 'ADV', 3: 'AUX', 4: 'CCONJ', 5: 'DET', 6: 'INTJ', 7: 'NOUN', 8: 'NUM', 9: 'PRON', 10: 'PROPN', 11: 'PUNCT', 12: 'SCONJ', 13: 'SPACE', 14: 'SYM', 15: 'VERB', 16: 'X'}\n",
      "Nombre de tags uniques : 17\n",
      "Exemple de mapping : {'ADJ': 0, 'ADP': 1, 'ADV': 2, 'AUX': 3, 'CCONJ': 4}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Récupérer tous les tags uniques\n",
    "all_tags = set(tag for example in train_dataset[\"pos_tags\"] for tag in example)\n",
    "tag2id = {tag: id for id, tag in enumerate(sorted(all_tags))}\n",
    "id2tag = {id: tag for tag, id in tag2id.items()}\n",
    "print(\"Mapping des labels :\", id2tag)\n",
    "\n",
    "num_labels = len(tag2id)\n",
    "\n",
    "print(f\"Nombre de tags uniques : {num_labels}\")\n",
    "print(\"Exemple de mapping :\", {k: tag2id[k] for k in list(tag2id)[:5]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7aab7c15-e633-4030-91b2-3e13d1b4ded4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3817782064f64b4cb8b60b966db5fedf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/133796 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "tokenizer = CamembertTokenizerFast.from_pretrained(\"camembert-base\")\n",
    "\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"pos_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:  # Token spécial ([CLS], [SEP], [PAD])\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Nouveau mot\n",
    "                label_ids.append(tag2id[label[word_idx]])\n",
    "            else:  # Sous-mot du même mot\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Appliquer à tout le dataset\n",
    "tokenized_train = train_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "tokenized_eval = eval_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "tokenized_test = test_dataset.map(tokenize_and_align_labels, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75151ad6-602c-4283-94b1-9225360d1e60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05a1e3bf-bffc-40af-8096-67903dbe5508",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "021510df-264c-48bd-ae1e-f9a695e17251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " GPU détecté : Quadro RTX 6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CamembertForTokenClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Modèle déplacé sur : cuda\n",
      "Début de l'entraînement sur : cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_56515/3144175555.py:86: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25089' max='25089' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25089/25089 56:47, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.172200</td>\n",
       "      <td>0.138877</td>\n",
       "      <td>0.954358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.126500</td>\n",
       "      <td>0.124076</td>\n",
       "      <td>0.958344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.105900</td>\n",
       "      <td>0.119750</td>\n",
       "      <td>0.959843</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=25089, training_loss=0.17180974402050028, metrics={'train_runtime': 3408.6761, 'train_samples_per_second': 117.755, 'train_steps_per_second': 7.36, 'total_flos': 4.593428184984254e+16, 'train_loss': 0.17180974402050028, 'epoch': 3.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    CamembertTokenizerFast,\n",
    "    CamembertForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForTokenClassification\n",
    ")\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 0. Sélection automatique GPU / CPU\n",
    "# ------------------------------------------------------------\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\" GPU détecté :\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\" Aucun GPU détecté → utilisation du CPU\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1. Charger le tokenizer\n",
    "# ------------------------------------------------------------\n",
    "tokenizer = CamembertTokenizerFast.from_pretrained(\"camembert-base\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2. Charger le modèle\n",
    "# ------------------------------------------------------------\n",
    "model = CamembertForTokenClassification.from_pretrained(\n",
    "    \"camembert-base\",\n",
    "    num_labels=num_labels\n",
    ").to(device)\n",
    "\n",
    "print(f\"✓ Modèle déplacé sur : {device}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3. Fonction compute_metrics pour accuracy\n",
    "# ------------------------------------------------------------\n",
    "def compute_metrics(pred):\n",
    "    predictions, labels = pred\n",
    "    preds = np.argmax(predictions, axis=-1)\n",
    "\n",
    "    mask = labels != -100\n",
    "    preds = preds[mask]\n",
    "    labels = labels[mask]\n",
    "\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4. Data collator\n",
    "# ------------------------------------------------------------\n",
    "data_collator = DataCollatorForTokenClassification(\n",
    "    tokenizer=tokenizer,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5. Arguments d'entraînement\n",
    "# ------------------------------------------------------------\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",     \n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "\n",
    "    fp16=torch.cuda.is_available(),   \n",
    "    report_to=\"none\",\n",
    "\n",
    "    load_best_model_at_end=True,      \n",
    "    metric_for_best_model=\"accuracy\", \n",
    "    greater_is_better=True,           \n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 6. Trainer\n",
    "# ------------------------------------------------------------\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_eval,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 7. Entraînement\n",
    "# ------------------------------------------------------------\n",
    "print(f\"Début de l'entraînement sur : {device}\")\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5ec72af6-c3c8-4748-9fb3-220925936916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Modèle et tokenizer sauvegardés dans : ./modele_20\n"
     ]
    }
   ],
   "source": [
    "# Sauvegarder le modèle et le tokenizer\n",
    "model_save_path = \"./modele_20\"\n",
    "trainer.save_model(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "\n",
    "print(f\"✓ Modèle et tokenizer sauvegardés dans : {model_save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8317ef-2c01-4dd3-afca-7a5a6d6573cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import classification_report\n",
    "\n",
    "predictions = trainer.predict(tokenized_test)\n",
    "preds = np.argmax(predictions.predictions, axis=2)\n",
    "\n",
    "# Convertir les IDs en tags\n",
    "pred_labels = [[id2tag[p] for p in pred if p != -100] for pred in preds]\n",
    "true_labels = [[id2tag[l] for l in label if l != -100] for label in tokenized_test[\"labels\"]]\n",
    "\n",
    "print(classification_report(true_labels, pred_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "744c69c9-b285-4ca5-9ed3-26ee86213b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Modèle et tokenizer chargés avec succès !\n"
     ]
    }
   ],
   "source": [
    "from transformers import CamembertForTokenClassification, AutoTokenizer\n",
    "\n",
    "# Chemin vers le dossier contenant votre modèle\n",
    "modele_path = \"../Youdas/modele_20\"\n",
    "\n",
    "# Charger le modèle\n",
    "model = CamembertForTokenClassification.from_pretrained(modele_path).to(\"cuda\")\n",
    "\n",
    "# Charger le tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(modele_path)\n",
    "\n",
    "print(\"✓ Modèle et tokenizer chargés avec succès !\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a0f0a98d-afc7-41f8-8588-500d48643d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble de phrases de test avec leurs étiquettes attendues\n",
    "test_phrases = [\n",
    "    (\"Le chat dort sur le tapis.\", [\n",
    "        (\"Le\", \"DET\"), (\"chat\", \"NOUN\"), (\"dort\", \"VERB\"),\n",
    "        (\"sur\", \"ADP\"), (\"le\", \"DET\"), (\"tapis\", \"NOUN\"), (\".\", \"PUNCT\")\n",
    "    ]),\n",
    "    (\"Marie mange une pomme verte.\", [\n",
    "        (\"Marie\", \"PROPN\"), (\"mange\", \"VERB\"), (\"une\", \"DET\"),\n",
    "        (\"pomme\", \"NOUN\"), (\"verte\", \"ADJ\"), (\".\", \"PUNCT\")\n",
    "    ]),\n",
    "    (\"Nous allons au parc demain.\", [\n",
    "        (\"Nous\", \"PRON\"), (\"allons\", \"VERB\"), (\"au\", \"ADP\"),\n",
    "        (\"parc\", \"NOUN\"), (\"demain\", \"ADV\"), (\".\", \"PUNCT\")\n",
    "    ]),\n",
    "    (\"Paris est une ville magnifique.\", [\n",
    "        (\"Paris\", \"PROPN\"), (\"est\", \"AUX\"), (\"une\", \"DET\"),\n",
    "        (\"ville\", \"NOUN\"), (\"magnifique\", \"ADJ\"), (\".\", \"PUNCT\")\n",
    "    ]),\n",
    "    (\"Oh, comme ce gâteau est délicieux !\", [\n",
    "        (\"Oh\", \"INTJ\"), (\",\", \"PUNCT\"), (\"comme\", \"ADV\"), (\"ce\", \"DET\"),\n",
    "        (\"gâteau\", \"NOUN\"), (\"est\", \"AUX\"), (\"délicieux\", \"ADJ\"),\n",
    "        (\"!\", \"PUNCT\")\n",
    "    ]),\n",
    "    (\"Apple a présenté l’iPhone 15 à New York.\", [\n",
    "        (\"Apple\", \"PROPN\"), (\"a\", \"AUX\"), (\"présenté\", \"VERB\"),\n",
    "        (\"l’\", \"DET\"), (\"iPhone\", \"PROPN\"), (\"15\", \"NUM\"),\n",
    "        (\"à\", \"ADP\"), (\"New\", \"PROPN\"), (\"York\", \"PROPN\"), (\".\", \"PUNCT\")\n",
    "    ]),\n",
    "]\n",
    "\n",
    "# Dictionnaire id2tag (à adapter si nécessaire)\n",
    "id2tag = {\n",
    "    0: 'ADJ', 1: 'ADP', 2: 'ADV', 3: 'AUX', 4: 'CCONJ',\n",
    "    5: 'DET', 6: 'INTJ', 7: 'NOUN', 8: 'NUM', 9: 'PRON',\n",
    "    10: 'PROPN', 11: 'PUNCT', 12: 'SCONJ', 13: 'SPACE',\n",
    "    14: 'SYM', 15: 'VERB', 16: 'X'\n",
    "}\n",
    "\n",
    "tag2id = {v: k for k, v in id2tag.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9e0bba29-49a4-4959-a630-e018cfb19f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import CamembertTokenizerFast, CamembertForTokenClassification\n",
    "\n",
    "# Charger le modèle et le tokenizer\n",
    "model_path = \"../Youdas/modele_20\"\n",
    "tokenizer = CamembertTokenizerFast.from_pretrained(model_path)\n",
    "model = CamembertForTokenClassification.from_pretrained(model_path).to(\"cuda\")\n",
    "\"\"\"\n",
    "def predire_pos(texte, model, tokenizer, id2tag):\n",
    "    inputs = tokenizer(texte.split(), return_tensors=\"pt\", truncation=True, is_split_into_words=True).to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    predictions = torch.argmax(outputs.logits, dim=2)\n",
    "    predicted_tags = [id2tag[id.item()] for id in predictions[0]]\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "    return tokens, predicted_tags\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "def predire_pos(texte, model, tokenizer, id2tag):\n",
    "    # Découpage propre : \"tapis.\" devient [\"tapis\", \".\"]\n",
    "    doc = nlp(texte)\n",
    "    mots = [token.text for token in doc]\n",
    "    \n",
    "    # Encodage avec is_split_into_words=True\n",
    "    inputs = tokenizer(mots, return_tensors=\"pt\", truncation=True, is_split_into_words=True).to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    predictions = torch.argmax(outputs.logits, dim=2)[0]\n",
    "    word_ids = inputs.word_ids(batch_index=0)\n",
    "    \n",
    "    aligned_tags = []\n",
    "    previous_word_idx = None\n",
    "    \n",
    "    for i, word_idx in enumerate(word_ids):\n",
    "        # On garde uniquement le premier sous-token de chaque mot (ignore les tokens spéciaux)\n",
    "        if word_idx is not None and word_idx != previous_word_idx:\n",
    "            aligned_tags.append(id2tag[predictions[i].item()])\n",
    "        previous_word_idx = word_idx\n",
    "        \n",
    "    return mots, aligned_tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "438e49dc-55a7-49d2-88bf-9dff989f0704",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regrouper_sous_tokens(tokens, tags):\n",
    "    mots_et_tags = []\n",
    "    current_token = None\n",
    "    current_tag = None\n",
    "\n",
    "    for token, tag in zip(tokens, tags):\n",
    "        if token.startswith(\"##\"):\n",
    "            if current_token:\n",
    "                current_token += token[2:]\n",
    "        else:\n",
    "            if current_token:\n",
    "                mots_et_tags.append((current_token.replace(\"▁\", \"\"), current_tag))\n",
    "            current_token = token.replace(\"▁\", \"\")\n",
    "            current_tag = tag\n",
    "    if current_token:\n",
    "        mots_et_tags.append((current_token, current_tag))\n",
    "\n",
    "    return mots_et_tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cfbfa245-aa31-4ea5-98b8-ba61ae9fcb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def calculer_precision(test_phrases, model, tokenizer, id2tag):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    for phrase, expected_tags in test_phrases:\n",
    "        mots, predicted_tags = predire_pos(phrase, model, tokenizer, id2tag)\n",
    "\n",
    "        # Aligner les mots prédits avec les mots attendus\n",
    "        for (mot_exp, tag_exp), tag_pred in zip(expected_tags, predicted_tags):\n",
    "            y_true.append(tag_exp)\n",
    "            y_pred.append(tag_pred)\n",
    "\n",
    "    precision = accuracy_score(y_true, y_pred)\n",
    "    return precision, y_true, y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c495e39f-40d6-4bdc-acf6-00f86f9dba27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Précision globale : 95.35%\n",
      "\n",
      "Phrase 1 : Le chat dort sur le tapis.\n",
      "Attendu\t\tPrédit\n",
      "Le (DET)\t\tLe (DET)\n",
      "chat (NOUN)\t\tchat (NOUN)\n",
      "dort (VERB)\t\tdort (VERB)\n",
      "sur (ADP)\t\tsur (ADP)\n",
      "le (DET)\t\tle (DET)\n",
      "tapis (NOUN)\t\ttapis (NOUN)\n",
      ". (PUNCT)\t\t. (PUNCT)\n",
      "\n",
      "Phrase 2 : Marie mange une pomme verte.\n",
      "Attendu\t\tPrédit\n",
      "Marie (PROPN)\t\tMarie (PROPN)\n",
      "mange (VERB)\t\tmange (VERB)\n",
      "une (DET)\t\tune (DET)\n",
      "pomme (NOUN)\t\tpomme (NOUN)\n",
      "verte (ADJ)\t\tverte (ADJ)\n",
      ". (PUNCT)\t\t. (PUNCT)\n",
      "\n",
      "Phrase 3 : Nous allons au parc demain.\n",
      "Attendu\t\tPrédit\n",
      "Nous (PRON)\t\tNous (PRON)\n",
      "allons (VERB)\t\tallons (VERB)\n",
      "au (ADP)\t\tau (ADP)\n",
      "parc (NOUN)\t\tparc (NOUN)\n",
      "demain (ADV)\t\tdemain (ADV)\n",
      ". (PUNCT)\t\t. (PUNCT)\n",
      "\n",
      "Phrase 4 : Paris est une ville magnifique.\n",
      "Attendu\t\tPrédit\n",
      "Paris (PROPN)\t\tParis (PROPN)\n",
      "est (AUX)\t\test (AUX)\n",
      "une (DET)\t\tune (DET)\n",
      "ville (NOUN)\t\tville (NOUN)\n",
      "magnifique (ADJ)\t\tmagnifique (ADJ)\n",
      ". (PUNCT)\t\t. (PUNCT)\n",
      "\n",
      "Phrase 5 : Oh, comme ce gâteau est délicieux !\n",
      "Attendu\t\tPrédit\n",
      "Oh (INTJ)\t\tOh (ADV)\n",
      ", (PUNCT)\t\t, (PUNCT)\n",
      "comme (ADV)\t\tcomme (SCONJ)\n",
      "ce (DET)\t\tce (DET)\n",
      "gâteau (NOUN)\t\tgâteau (NOUN)\n",
      "est (AUX)\t\test (AUX)\n",
      "délicieux (ADJ)\t\tdélicieux (ADJ)\n",
      "! (PUNCT)\t\t! (PUNCT)\n",
      "\n",
      "Phrase 6 : Apple a présenté l’iPhone 15 à New York.\n",
      "Attendu\t\tPrédit\n",
      "Apple (PROPN)\t\tApple (PROPN)\n",
      "a (AUX)\t\ta (AUX)\n",
      "présenté (VERB)\t\tprésenté (VERB)\n",
      "l’ (DET)\t\tl’ (DET)\n",
      "iPhone (PROPN)\t\tiPhone (PROPN)\n",
      "15 (NUM)\t\t15 (NUM)\n",
      "à (ADP)\t\tà (ADP)\n",
      "New (PROPN)\t\tNew (PROPN)\n",
      "York (PROPN)\t\tYork (PROPN)\n",
      ". (PUNCT)\t\t. (PUNCT)\n"
     ]
    }
   ],
   "source": [
    "precision, y_true, y_pred = calculer_precision(test_phrases, model, tokenizer, id2tag)\n",
    "\n",
    "print(f\"Précision globale : {precision:.2%}\")\n",
    "\n",
    "# Afficher les résultats détaillés\n",
    "for i, (phrase, expected_tags) in enumerate(test_phrases):\n",
    "    print(f\"\\nPhrase {i+1} : {phrase}\")\n",
    "    tokens, predicted_tags = predire_pos(phrase, model, tokenizer, id2tag)\n",
    "    predicted_grouped = regrouper_sous_tokens(tokens, predicted_tags)\n",
    "\n",
    "    print(\"Attendu\\t\\tPrédit\")\n",
    "    for (mot_exp, tag_exp), (mot_pred, tag_pred) in zip(expected_tags, predicted_grouped):\n",
    "        print(f\"{mot_exp} ({tag_exp})\\t\\t{mot_pred} ({tag_pred})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441e7d19-3c07-4951-84da-178b2f64407c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e65a3b-9890-4a9c-a2dc-f0c930d70770",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
