# config.yaml
seed_everything: 1

trainer:
  max_epochs: 10
  accelerator: "gpu"
  devices: [0]
  precision: "16-mixed" 
  gradient_clip_val: 0.0
  
  accumulate_grad_batches: 9
  log_every_n_steps: 10
  default_root_dir: "./../checkpoints/"
  
  callbacks:
    - class_path: pytorch_lightning.callbacks.ModelCheckpoint
      init_args:
        save_top_k: 10
        monitor: "train_loss"
        mode: "min"
        save_last: true
        filename: "best-{epoch}-{step}-{train_loss:.4f}"

model:
  learning_rate: 0.00015
  vocab_size: 32000
  hidden_size: 768
  num_hidden_layers: 12
  num_attention_heads: 12

data:
  batch_size: 28         
  tokenizer_path: "/home/camembert/js/tokenizer.json"
  
  num_workers: 28
  
  train_files:
    - "/home/camembert/dataset_g5/fr_part_1.txt"
    - "/home/camembert/dataset_g5/fr_part_2.txt"
    - "/home/camembert/dataset_g5/fr_part_3.txt"