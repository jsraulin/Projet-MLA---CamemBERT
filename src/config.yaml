# config.yaml
seed_everything: 1

trainer:
  max_epochs: 10              # 10 pour test
  accelerator: "auto"         # Utilise GPU si dispo, sinon CPU
  devices: 1
  precision: "16-mixed"       # Utilise moins de VRAM (mixed precision)
  accumulate_grad_batches: 4  # Simule un batch size 4x plus grand
  log_every_n_steps: 10
  default_root_dir: "./../checkpoints/" # Où sauvegarder le modèle

model:
  learning_rate: 0.0001
  vocab_size: 32005           # Taille vocabulaire standard CamemBERT
  hidden_size: 768
  num_hidden_layers: 6        # Tester 6 couches, puis 12

data:
  batch_size: 8               # selon VRAM, baisser à 4 si OOM
  train_files:                # Les train files qui manquait
    - "./DATASET_pour_camemBERT/fr_part_1.txt"
    - "./DATASET_pour_camemBERT/fr_part_2.txt"
    - "./DATASET_pour_camemBERT/fr_part_3.txt"